{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Notas examen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Dividir los datos en train y test. Esto para entrenar tu modelo con unos datos y probarlo en datos que no ha visto y ver que tan bueno fue. Es con el objetivo de evitar overfitting, o sea que el modelo pueda generalizar y no memorizar los datos.\n",
    "\n",
    "---\n",
    "\n",
    "## Teorema Frisch-Waugh-Lovell\n",
    "\n",
    "$$\n",
    "\\hat y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\n",
    "$$\n",
    "Esta es la regresión lineal original con todas las variables, y se busca encontrar el efecto causal de la variable $x_1$.\n",
    "\n",
    "$$\n",
    "\\hat y = \\theta_0 + \\theta_2 x_2 + \\theta_3 x_3\n",
    "$$\n",
    "Para ello haces la regresión lineal para $y$ sin utilizar $x_1$.\n",
    "\n",
    "$$\n",
    "y - \\hat y = \\text{residuales y}\n",
    "$$\n",
    "Y obtienes los residuales entre tu regresión sin $x_1$ y los valores reales de $y$. Estos residuales de $y$ es lo que no pueden explicar $x_2$ y $x_3$ de $y$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat {x_1} = \\gamma_0 + \\gamma_2 x_2 + \\gamma_3 x_3\n",
    "$$\n",
    "Posteriormente realizas una regresión lineal para $x_1$ con las variables $x_2$ y $x_3$.\n",
    "\n",
    "$$\n",
    "x_1 - \\hat {x_1} = \\text{residuales x}\n",
    "$$\n",
    "Y calculas los residuales entre tu predicción de $\\hat {x_1}$ y $x_1$ real. Estos residuales de $x_1$ es lo que no pueden explicar  $x_2$ y $x_3$ de $x_1$\n",
    "\n",
    "$$\n",
    "y - \\hat y = \\beta_1(x_1 - \\hat {x_1})\n",
    "$$\n",
    "Por útlimo realizas un regresión de los residuales de $y$ contra los residuales de $x_1$, y la $B_1$ resultante es la misma que la $B_1$ de la regresión original, lo cual sería el efecto causal de $x_1$ sobre $y$.\n",
    "\n",
    "---\n",
    "\n",
    "## R2\n",
    "\n",
    "Es el porcentaje de la variación de los datos explicada por nuestro modelo. Suele estar entre 0 y 1, mientras es más cercana a 1 significa que el modelo explica bien la variación de los datos.\n",
    "\n",
    "---\n",
    "\n",
    "## Descenso en gradiente\n",
    "\n",
    "Proceso iterativo el cual busca minimizar tu error (función de pérdida) actualizando los valores de las $\\beta$'s de tu regresión a través de derivadas parciales.\n",
    "\n",
    "Tienes la función de pérdida: $L = \\frac{1}{2} \\sum (\\hat y - y)^2$\n",
    "\n",
    "Para este ejemplo $\\hat y = \\beta_0 + \\beta_1 x_1$\n",
    "\n",
    "Tu buscas:\n",
    "$$\n",
    "min \\quad \\frac{1}{2} \\sum (\\hat y - y)^2\n",
    "$$\n",
    "\n",
    "Para eso realizas de forma iterativa lo siguiente ($\\alpha$ es tasa de aprendizaje): $\\beta_1 = \\beta - \\alpha \\frac{\\partial L}{\\partial \\beta}$\n",
    "\n",
    "Cada $\\beta$ se actualiza restándole la derivada parcial de $L$ multiplicada por la tasa de aprendizaje $\\alpha$, la cual ayuda a que el cambio en la $\\beta$ no sea tan grande. Esta nueva $\\beta$ se utiliza para recalcular la función de pérdida y se vuelve a actualizar en cada iteración, restando nuevamente la derivada parcial multiplicada por la tasa de aprendizaje. Este proceso se repite hasta que el modelo converge.\n",
    "\n",
    "El método funciona porque al hacer la resta, el signo negativo invierte la dirección de la pendiente obtenida en la derivada después de cada iteración, ayudando a la convergencia."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
