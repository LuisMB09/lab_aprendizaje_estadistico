{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71470336",
   "metadata": {},
   "source": [
    "# Shap Values\n",
    "\n",
    "Se hacen todas las permutaciones posibles de las variables de tu dataset y se calcula la contribución marginal de cada una de las variables en cada una de estas permutaciones. Por último, se promedian estas contribuciones marginales de cada variables para obtener su shap value.\n",
    "\n",
    "La contribución marginal de la variable representa como cambia la predicción de un modelo al agregar una nueva variable.\n",
    "\n",
    "Partes de un modelo base sin variables (promedio) y vas agregando una variable a la vez donde en cada paso obtienes su contribución marginal. Por lo tanto la predicción de un modelo es explicada por: $f(x_1,x_2,x_3) = \\text{modelo base} + SHAPx_1 + SHAPx_2 + SHAPx_3$. Esto porque los shap values representan las contribuciones de las variables al modelo.\n",
    "\n",
    "---\n",
    "\n",
    "# KMeans\n",
    "\n",
    "Algoritmo utilizado para hacer clusters de tus datos, donde los puntos que se encuentran en el mismo cluster son lo más parecidos entre si. La manera en la que hace los clusters es utilizando las distancias entre los puntos a ciertos centroides. Se incia con K número de centroides (número de clusters) aleatorios, se asigna cada uno de los puntos al centro incial más cercano y después se calculan nuevos centorides como el promedio de los puntos asignados, se repite el proceso de asignar puntos a estos nuevos centroides y se calculan nuevos centro en cada iteración cambiando así los puntos asignados a cada cluster hasta que los centroides ya no cambian.\n",
    "\n",
    "Se calcula la distancia euclidiana al cuadrado entre cada punto y el centroide para asignarlo al más cercana:\n",
    "\n",
    "$$\n",
    "\\|x_i - \\mu_k\\|^2 = \\sum_{j=1}^n (x_{ij} - \\mu_{kj})^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# PCA\n",
    "\n",
    "Técnica que busca reducir la dimensionalidad de los datos mientras se mantiene la mayor cantidad de varianza de los datos posible. Para ello necesitas la proyección sobre un componente la cual se expresa como:\n",
    "\n",
    "$$\n",
    "\\text{Proy}_{u_1}(x_i) = u_1^T x_i\n",
    "$$\n",
    "\n",
    "Se tiene como objetivo que los componentes expliquen la mayor cantidad de varianza la cual se calcula de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\text{Varianza} = \\frac{1}{N} \\sum_{n=1}^N \\left( u_1^T x_n - u_1^T \\bar{x} \\right)^2\n",
    "$$\n",
    "\n",
    "Agrupando términos:\n",
    "\n",
    "$$\n",
    "= u_1^T S u_1\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $S$ es la matriz de covarianza de los datos.\n",
    "\n",
    "Aquí $u_1$ es la dirección del primer componente principal y buscas proyectar los datos para capturar la mayor varianza posible para explicar más parte de los datos. Para esto se debe de maximizar la varianza para lo que se utilizan los multiplicadores de Lagrange para resolver el problema de optimización:\n",
    "\n",
    "$$\n",
    "\\max_{u_1} u_1^T S u_1 \\\\\n",
    "\n",
    "\\text{s.t.} \\quad \\|u_1\\| = 1\n",
    "$$\n",
    "\n",
    "Al resolver con los multiplicadores de Lagrange tienes que:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(u_1, \\lambda) = u_1^T S u_1 - \\lambda (u_1^T u_1 - 1)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial u_1} = 2 S u_1 - 2 \\lambda u_1 = 0 \\\\\n",
    "S u_1 = \\lambda u_1\n",
    "$$\n",
    "\n",
    "La expresión a la que se llega es la definición de un problema de eigenvalores donde $u_1$ es un eigenvector de de la matriz S y $\\lambda$ es el eigenvalor. Por lo tanto los eigenvalores que son la solución de la expresión obtenida son los valores que maximizan la varianza asociada a la componente principal explicando así una mayor cantidad de varianza de los datos por lo que se reduce la dimensionalidad de datos manteniendo la información.\n",
    "\n",
    "---\n",
    "\n",
    "# Tratamiento Continuo de S learner\n",
    "\n",
    "Entrenas tu modelo sobre los datos originales. Posteriormente sobre una obervación de tus datos defines sobre que variable continua vas a analizar su efecto. A esta variable le das multiples valores distintos y de forma iterativa usas tu modelo para hacer predicciones con los distintos valores y ver que efecto tiene sobre la predicción. Por ejemplo como distintos precios de un producto podrían afectar las posibles ventas.\n",
    "\n",
    "---\n",
    "\n",
    "# CATE\n",
    "\n",
    "Efecto esperado del tratamiento para los individuos con dicha característica.\n",
    "\n",
    "---\n",
    "\n",
    "# S learner (Discreto)\n",
    "\n",
    "Entrenas tu modelo con los datos originales, posteriormente modificas tus datos para que todos tengan el tratamiento (1) y también para que nadie tenga tratamiento (0), estos datos los utilizas para hacer predicciones donde a las predicciones con tratamiento le restas las predicciones sin tratamiento, y la diferencia obtenida representa el CATE de tener el trtamiento.\n",
    "\n",
    "---\n",
    "\n",
    "# T learner\n",
    "\n",
    "Tus datos los modificas para tener dos datasets diferentes, uno donde todos tienen el tratamiento (1) y otro donde nadie tiene el tratamiento (0). Con estos datos entrenas dos modelos diferentes, uno con los datos con el tratamiento (model_t1) y otro con los datos sin tratamiento (model_t0). Posteriormente usas ambos modelos para hacer las predicciones con tus datos de test originales, donde un modelo tiene un sesgo hacia el tratamiento y el otro hacia los que no lo tienen por los datos con los que se entranaron.\n",
    "\n",
    "Por último las predicciones realizadas por el model_t1 le restas las predicciones de model_t0 y la diferencia entre esta spredicciones es el CATE provocado por el tratamiento."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
