{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1ac6b9",
   "metadata": {},
   "source": [
    "## √Årbol de decisi√≥n\n",
    "\n",
    "Un modelo el cual utiliza varios niveles, donde en cada nivel existe una caractr√≠stica que se evalua a trav√©s de un umbral, va dividiendo los datos en los que son mayor o menor al umbral definido en la caracter√≠stica.\n",
    "\n",
    "\n",
    "### Regresi√≥n\n",
    "\n",
    "Este algoritmo usa todas las variables y los splits posibles (umbrales) y elige el umbral en cada variable que reduzca en mayor medida la varianza.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Reducci√≥n de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Varianza de cada rama ponderada por el n√∫mero de gente de cada una de las ramas.\n",
    "\n",
    "### Clasificaci√≥n\n",
    "\n",
    "De igual forma que en la regresi√≥n, el modelo usa todas las variables y prueba todos los umbrales posibles, eligiendo el que cause una mayor reducci√≥n de impureza, la cual se calcula a trav√©s del Gini o de la Entriop√≠a.\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum p_i^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entrop√≠a = -\\sum p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Impureza} = \\text{Impureza Nodo Padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Impureza de cada rama ponderada por el n√∫mero de personas en cada una de ellas.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Utilizas la t√©cnica de bootsrap para hacer muestreos aleatorios de tus datos, sobre los cuales entrenas tu modelo de √°rbol de decisi√≥n. Cada uno de estos modelos realiza sus predicciones, y el promedio de estas ser√≠a la predicci√≥n utilizada por el algoritmo de Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Boosting\n",
    "\n",
    "Proceso iterativo el cual mejora los errores cometidos en pasos anteriores. Inicia con una predicci√≥n a la cual le calcula el error, despu√©s se entrena un nuevo √°rbol sobre estos errores, con esto se actualiza la predicci√≥n del modelo total. Esto se hace a trav√©s de un factor de aprendizaje que pondera las predicciones anteriores y se va repitiendo el proceso.\n",
    "\n",
    "Cada vez usa el error anterior para en cada iteraci√≥n ir haciendo est√© m√°s peque√±o. La predicci√≥n del √°rbol anterior es el promedio de los residuales obtenidos en cada una de las hojas. Modelo nuevo = modelo anteriror + v (nuevo √°rbol). Para clasificaci√≥n se utilizan las log-odds.\n",
    "\n",
    "---\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "Sumas √°rboles √≥ptimos.\n",
    "\n",
    "XGBoost construye un modelo final a trav√©s de una suma de √°rboles.\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\hat{y}_i^{(t)}$ es la predicci√≥n.\n",
    "- $f_t(x_i)$ es el nuevo √°rbol.\n",
    "\n",
    "En cada una de las iteraciones se busca minimizar la contribuci√≥n marginal a la funci√≥n de p√©rdida total.\n",
    "\n",
    "La funci√≥n de p√©rdida se compone de dos partes, una que son los √°rboles anterirores y otra parte que representa el √°rbol actual. La parte de los √°rboles anteriores se usa cmo constante (ya esta dado) y se optimiza lo que aporta el nuevo √°rbol, llegando a:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} =\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) + \\sum_{k=1}^{t} \\Omega(f_k)\n",
    "$$\n",
    "\n",
    "$\\Omega(f_k)$ es un factor de regularizaci√≥n el cual castiga √°rboles m√°s complejos.\n",
    "\n",
    "Como la funci√≥n de p√©rdida es compleja, se utiliza las series de Taylor para poder aproximar la funci√≥n de p√©rdida cerca de las predicciones. La serie de Taylor de segundo orden permite aproximar la funci√≥n de p√©rdida con una forma cuadr√°tica, la cual es m√°s sencilla de optmizar y velve m√°s eficiente el proceso. Utilizando esta expansi√≥n, el gradiente y el hessiano la funci√≥n de p√©rdida queda de la siguiente forma:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Con ello la optimizaci√≥n es un problema cuadr√°tico m√°s sencillo de optimizar.\n",
    "\n",
    "Cada nuevo √°rbol $f_t$ asigna un valor constante $w_j$. Entonces:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "Con:\n",
    "- $G_j = \\sum_{i \\in R_j} g_i$\n",
    "- $H_j = \\sum_{i \\in R_j} h_i$\n",
    "\n",
    "Despu√©s minimizas la funci√≥n de p√©rdida con respecto de $w_j$ para obtener el output √≥ptimo por hoja:\n",
    "\n",
    "$$\n",
    "\\textbf{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Con estos valores √≥ptimos calculas el similarity score para saber cuanto mejora la p√©rdida en cada una de loas hojas (nos dice la calidad de la hoja):\n",
    "\n",
    "$$\n",
    "\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Cuando calculas el similarity score lo utilizas para obtener el gain, el cual nos dice si vale la pena hacer un split (aumentar la complejidad del √°rbol).\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\left( \\text{Similarity}_\\text{izq} + \\text{Similarity}_\\text{der} - \\text{Similarity}_\\text{padre} \\right) - \\gamma\n",
    "$$\n",
    "\n",
    "La predicci√≥n final se obtiene sumando todos los √°rboles:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = F_0(x_i) + \\sum_{t=1}^{M} f_t(x_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Comparativa de modelos\n",
    "\n",
    "La estructura del √°rbol:\n",
    "\n",
    "* XGBoost produce √°rboles m√°s sim√©tricos y balanceados.\n",
    "\n",
    "* LightGBM produce √°rboles m√°s profundos y desbalanceados si no se controla.\n",
    "\n",
    "La precisi√≥n y riesgo de overfitting:\n",
    "\n",
    "* Leaf-wise (LightGBM) puede encontrar mejores divisiones, pero se sobreajusta m√°s f√°cil.\n",
    "\n",
    "* Level-wise (XGBoost) es m√°s estable, pero a veces menos preciso.\n",
    "\n",
    "| Caracter√≠stica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | R√°pido, pero m√°s lento que LightGBM y CatBoost             | üî• Muy r√°pido gracias a histogramas y leaf-wise growth      | R√°pido, aunque un poco m√°s lento que LightGBM                  |\n",
    "| **Precisi√≥n**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categ√≥ricas                        |\n",
    "| **Variables categ√≥ricas**   | ‚ùå No las maneja (requiere encoding manual)                | ‚ùå No las maneja (requiere encoding manual)                 | ‚úÖ Soporte nativo + regularizaci√≥n secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ‚úÖ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ‚úÖ Autom√°tico                                               | ‚úÖ Autom√°tico                                                | ‚úÖ Autom√°tico                                                   |\n",
    "| **Soporte GPU**             | ‚úÖ S√≠ (bastante estable)                                   | ‚úÖ S√≠ (muy r√°pido)                                           | ‚úÖ S√≠ (algo m√°s limitado)                                      |\n",
    "| **Instalaci√≥n**             | F√°cil (`pip install xgboost`)                             | F√°cil (`pip install lightgbm`)                              | Un poco m√°s pesada (`pip install catboost`)                   |\n",
    "| **Documentaci√≥n**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacci√≥n con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ‚úÖ Neutral                                                  | ‚úÖ Neutral                                                   | ‚ö†Ô∏è Sensible (por codificaci√≥n secuencial)                      |\n",
    "\n",
    "Lightgbm funciona a trav√©s de bins, mientras que catboost deja las mismas ramas, lo cual hace una predicci√≥n m√°s veloz.\n",
    "\n",
    "Promedio de tu variable de forma secuancial (categorical encoding). Usa el promedio conocido hasta ese momento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
