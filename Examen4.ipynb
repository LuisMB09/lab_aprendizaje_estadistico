{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4fb4a27-9f28-435e-8a52-8a9351c7d47d",
   "metadata": {},
   "source": [
    "# Examen 4\n",
    "\n",
    "## Sección 1: Interpretabilidad con SHAP (25 puntos)\n",
    "\n",
    "**1.1** (Teoría, 10 pts)  \n",
    "**¿Qué representa un valor SHAP en el contexto de un modelo de machine learning? ¿Cuál es su fundamento teórico?**\n",
    "\n",
    "Es el promedio de las contribuciones marginales de cada variable después de haber realizado todas las permutaciones posibles de tus variables. Explican como llegar de tu valor esperado (modelo base) hasta la predicción realizada. Dice cuanto aporta la variable para llegar a la predicción del modelo.\n",
    "\n",
    "$$f(x_1,x_2,x_3) = \\text{modelo base} + SHAPx_1 + SHAPx_2 + SHAPx_3$$\n",
    "\n",
    "El fundamento teórico viene de la teoría de juegos cooperativos y asignan de manera justa la predicción entre cada una de las variables del modelo. Donde u fórmula general es la siguiente:\n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum_{S \\subseteq N \\setminus \\{i\\}} \\frac{|S|! \\cdot (n - |S| - 1)!}{n!} \\cdot \\left[ f(S \\cup \\{i\\}) - f(S) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "$\\phi_i$ = shap value\n",
    "\n",
    "$f(S \\cup \\{i\\}) - f(S)$ = contribución marginal de cada una de las variables.\n",
    "\n",
    "$\\frac{|S|! \\cdot (n - |S| - 1)!}{n!}$ = peso asignado a las variables para ponderar de forma justa según en l aposición que aparezca dentro de las permutaciones.\n",
    "\n",
    "**1.2** (Cálculo, 10 pts)  \n",
    "Un modelo predice que un cliente tendrá una probabilidad de impago del 0.78. El valor esperado del modelo es 0.5. Los SHAP values para tres variables son:  \n",
    "- Edad: +0.10  \n",
    "- Ingreso mensual: -0.05  \n",
    "- Historial crediticio: +0.23  \n",
    "\n",
    "**¿La suma es consistente con la predicción? Explica.**\n",
    "\n",
    "$0.5 + 0.1 - 0.05 + 0.23 = 0.78$ \n",
    "\n",
    "Por lo que la suma es consistente pues se pasa adecuadamente del valor esperado al predecido a través de las contribuciones marginales de las variables. Donde los shap values de cada variable muestran claramente cuanto aportan para la predicción. En este caso el Ingreso mensual reduce la probabilidad de impago mientras que su edad e historial crediticio la aumentan.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 2: K-Means Clustering (20 puntos)\n",
    "\n",
    "**2.1** (Teoría, 10 pts)  \n",
    "**Explica que hace KMeans, el algoritmo, como encuentra clusters, etc.**\n",
    "\n",
    "Algoritmo utilizado para hacer clusters de tus datos, donde los puntos que se encuentran en el mismo cluster son lo más parecidos entre si. La manera en la que hace los clusters es utilizando las distancias entre los puntos a ciertos centroides. Se incia con K número de centroides (número de clusters) aleatorios, se asigna cada uno de los puntos al centro incial más cercano con la distancia euclidiana al cuadrado entre el punto y el centroide más cercano:\n",
    "\n",
    "$$\n",
    "\\|x_i - \\mu_k\\|^2 = \\sum_{j=1}^n (x_{ij} - \\mu_{kj})^2\n",
    "$$\n",
    "\n",
    "Después se calculan nuevos centorides como el promedio de los puntos asignados, se repite el proceso de asignar puntos a estos nuevos centroides y se calculan nuevos centro en cada iteración cambiando así los puntos asignados a cada cluster hasta que los centroides ya no cambian.\n",
    "\n",
    "**2.2** (Criterio, 10 pts)  \n",
    "**Explica cómo usarías el método del codo (*elbow method*) y qué limitaciones tiene.**\n",
    "\n",
    "La gráfica del codo la utilizas para obtener el número de cluster \"óptimos\" justo donde está el codo de la gráfica, ya que a partir de ese punto por cada cluster extra la reducción de inercia es cada vez menor (menor pendiente), de esta forma evitas una cantidad de clusters alta innecesaria y mantienes clusters que sean relevantes con buena información y que hayan reducido una buena cantidad de iniercia.\n",
    "\n",
    "Al ser un método visual una limitante importante es en gráficas con curvas suaves donde no sea evidente el codo lo cual lo vuelve un método subjetivo donde cada quien tiene su criterio visual, además puedes perder infromación valiosa al clasificar si se tienen pocos clusters y no necesariamente garantiza una buena sepración de clusters con los elegidos si exosten relaciones complejas entre las variables.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 3: PCA – Análisis de Componentes Principales (20 puntos)\n",
    "\n",
    "**3.1** (Teoría, 5 pts)  \n",
    "**¿Qué significa que la primera componente principal maximiza la varianza?**\n",
    "\n",
    "Busca el vector en la dirección la cual los datos proyectados hacia el vector tengan la mayor varianza. Esto se hace ya que buscas que las compenentes principales puedan explicar la mayor varianza posible de los datos, por lo tanto buscas maximizarla.\n",
    "\n",
    "Para encontrar este vector se hace resolviendo un problema de optmización con multiplicadores de Lagrange:\n",
    "\n",
    "$$\n",
    "\\max_{u_1} u_1^T S u_1 \\\\\n",
    "\n",
    "\\text{s.t.} \\quad \\|u_1\\| = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(u_1, \\lambda) = u_1^T S u_1 - \\lambda (u_1^T u_1 - 1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial u_1} = 2 S u_1 - 2 \\lambda u_1 = 0 \\\\\n",
    "S u_1 = \\lambda u_1\n",
    "$$\n",
    "\n",
    "Esta última expresión es el problema de eigenvectores y eigenvalores, por lo que al resolverla y tener los eigenvalores logras encontrar el vector que maximiza la varianza proyectada de los datos.\n",
    "\n",
    "**3.2** (Cálculo, 10 pts)  \n",
    "**Si tienes 10 variables correlacionadas y aplicas PCA, ¿cuántas componentes necesitas para explicar al menos el 90% de la varianza? Describe cómo se respondería esta pregunta**\n",
    "\n",
    "Una vez aplicado el PCA tendrías un dataset nuevo con estas variables. Posteriormente calculas la varianza que explica de manera individual cada una de las PCA. Con estos vas calculando la varianza acumulada sumando la varianza da cada PCA en orden hasta llegar al 90%. Con esto se sabe cuantas variables PCA explican este porcentaje de varianza.\n",
    "\n",
    "Se pude expresar:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n Var_i\n",
    "$$\n",
    "\n",
    "A través de código sería de la siguiente manera (suponiendo que ya tienes un pipeline):\n",
    "\n",
    "```python\n",
    "objeto_pca = pipeline.named_steps['pca']\n",
    "\n",
    "for i in range(1, len(objeto_pca.explained_variance_ratio_)):\n",
    "    print(f'Componente {i+1}: {objeto_pca.explained_variance_ratio_[i]}')\n",
    "\n",
    "```\n",
    "\n",
    "Y vas sumando las varianzas hasta llegar a 90% para saber el número de PCA necesarias.\n",
    "\n",
    "**3.3** (Aplicación, 5 pts)  \n",
    "**¿Tiene sentido usar PCA y usar todas las componentes para predecir? Justifica tu respuesta.**\n",
    "\n",
    "No ya que el objetivo de usar las PCA es reducir la dimensionalidad de los datos para tener tiempos de entrenaiento más eficente para los modelos conservando gran parte de la varianza. Si usas todas las PCA olvidas este objetivo ya que usarías todas las variables, además de que los resultados del modelo serían prácticamente idénticos que si utilizas todas las variables originales.\n",
    "\n",
    "---\n",
    "\n",
    "## Sección 4: Causalidad y Meta-Learners (35 puntos)\n",
    "\n",
    "**4.1** (Conceptual, 10 pts)  \n",
    "**Define el estimando CATE y da un ejemplo de aplicación en negocios.**\n",
    "\n",
    "Es el efecto causal esperado que tiene un tratamiento para observaciones de tus datos con cierta característica. Muestra como afecta tener o no cierta característica para la predicción.\n",
    "\n",
    "Puede tener diversas aplicaciones en negocios, por ejemplo:\n",
    "\n",
    "- Como afecta el precio del producto a las ventas.\n",
    "- Como afecta si una tienda tiene promociones a las ventas.\n",
    "- Como afecta si una persona tiene hijos para su probabilidad de impago de un crédito.\n",
    "\n",
    "**4.2** (Comparación, 15 pts)  \n",
    "**Explica las diferencias entre los siguientes enfoques para estimar CATE:**\n",
    "- S-Learner  \n",
    "\n",
    "Se podría decir que es el método más sencillo, donde entrenas tu modelo con los datos originales y luego lo usas para hacer predicciones con tus datos modificados donde todos tienen el tratamiento (1) y donde nadie lo tiene (0). El CATE es la resta de las predicciones con tratamiento menos las predicciones sin tratamiento.\n",
    "\n",
    "    - Ventajas: Es un método simple y aprovechas toda tu información.\n",
    "    - Desventajas: Los resultados dependen de lo bueno del modelo base y si el modelo considera poco relevante la variable de tratamiento, el CATE podría no calcularse correctamente ya que no se le da el peso suficiente a la variable.\n",
    "\n",
    "- T-Learner  \n",
    "\n",
    "Su enfoque es al revés que el S-Learner, donde entrenas dos modelos con datos modificados, uno donde todos tienen el tratamiento (model_t1) y otro donde nadie tiene el tratamiento (model_t0). Posteriormente relizas predicciones con ambos modelos usando los datos originales y la resta entre predicciones de model_t1 - model_t0 son el CATE obtenido.\n",
    "\n",
    "    - Ventajas: Es específico para cada grupo por que hace un modelo para gente con y sin tratamiento. Al cambiar solo el tratamiento la diferencia entre predicciones captura directamente el CATE.\n",
    "    - Desventajas: Al usar modelo con datos distintos en el tratamiento estos pueden no dar resultados tan buenos.\n",
    "\n",
    "- X-Learner  \n",
    "\n",
    "Es el método más complejo donde usa model_t1 y model_t0 del T-Leaner, con estos modelos se van a calcular D0 y D1. Para D0 calculas lo que paso menos lo que pasaría sin tratamiento donde a y_train le restas la predicción de model_t0 usando su contrafactual. Pra D1 obtienes lo que habría pasado menos lo que paso donde usas las predicciones del model_t1 con los datos de su contrafactual menos lo que paso. Tanto D0 como D1 son el efecto causal del contrafactual de cada grupo, es decir, usas lo que hubiera pasado.\n",
    "\n",
    "Ya que tienes calculados D0 y D1 se entranan dos nuevos modelos los cuales serán utilizados para predecir el efecto causal (CATE) condicional dentro de cada grupo, es decir, calcula el CATE para los grupos con tratmiento y sin tratamiento. Luego usas un modelo de clasificación para tener el propensity score (probabilidad de tener el tratamiento en tus datos) la cual se utiliza para ponderar de manera correcta el CATE de los modelos.\n",
    "\n",
    "    - Ventajas: En caso de funcionar es el mejor. Estimación más robusta y completa del CATE al cruzar datos con y sin tratamiento a través de distintos modelos.\n",
    "    - Desventajas: Proceso más complejo y pude ser tardado con muchos datos. Al usar múltiples modelos es muy sensible a fallos de modelos que no realicen un buena predicción para los datos.\n",
    "\n",
    "**Incluye ventajas y desventajas.**\n",
    "\n",
    "**4.3** (Implementación, 10 pts)  \n",
    "**Te entregan un dataset con una columna `treatment` (0/1), un `outcome`, y varias covariables. ¿Cómo entrenarías un X-Learner paso a paso?**\n",
    "\n",
    "Dividir datos en train y test.\n",
    "\n",
    "Inicias con un T-Learner, donde tienes dos datasets, uno donde todos tienen tratamiento y otro donde nadie lo tiene. Con esto entranas dos modelos model_t0 y model_t1. Con estos modelos calculas D0 y D1 que son la diferencia entra lo que paso y lo que hubiera pasado, esto se hace a través de predicciones de los modelos con su contrafactual, es decir, para model_t0 usas los datos con todos treatment y para model_t1 usas los datos donde nadie tiene treatment.\n",
    "\n",
    "Utilizas D0 y D1 para entrenar los modelos que predicen el CATE como variable `y`. En este caso entrenas md0 con los datos con treatment como `X` y D0 como `y`, para md1 es al revés, usas los datos sin treatment como `X`y D1 como `y`. Posteriormente calculas el propensity score, donde con los datos originales haces un modelo de clasificación para predecir la probabilidad de tener el tratamiento, esto lo haces usando tus datos sin la columna de treatment como `X` y la columna treatment como `y`.\n",
    "\n",
    "Por último calculas las predicciones de md1 y md0 con los datos de test originales, y estás predicciones las ponderas a través del propensity score para calcular el CATE.\n",
    "\n",
    "```python\n",
    "cate = propensity_score * predict_d1 + (1 - propensity_score) * predict_d0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
